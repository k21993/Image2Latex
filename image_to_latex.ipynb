{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"IMAGE TO LATEX CONVERSION\"\"\"\n",
    "\n",
    "###import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import seq2seq\n",
    "from seq2seq import  AttentionSeq2Seq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first image is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x120fdf60>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x8gZ4LdfFKg4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6c1cac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Reading in pre-processed images\"\"\"\n",
    "\n",
    "###Pre- Processing carried out by using preprocess_images.py written by the authors of the IM2LATEX\n",
    "PATH1 = \"C:/Users/z003tp7e/Desktop/p\"\n",
    "PATH2= \"C:/Users/z003tp7e/Desktop/im2latex_data/im2latex_formulas.lst\"\n",
    "\n",
    "images = {} #dictionary mapping from- name of image: image matrix\n",
    "\n",
    "\n",
    "for i,filename in enumerate(os.listdir(PATH1)):\n",
    "    if (i<100):\n",
    "        \n",
    "        img = Image.open(\"C:/Users/z003tp7e/Desktop/p/\" + filename) ##0 param reads the image as gray-scale--> IMREAD(F, 0)\n",
    "        #if img is not None:\n",
    "        ###reshape images to size(800, 800, 3), the max width and height for train,test, and val is 800\n",
    "\n",
    "        size= 100, 100               ##MOST IMAGES ARE UNDER THIS SIZE...REDUCE/INCREASE THIS IF NECESSARY\n",
    "        img = img.resize(size, Image.ANTIALIAS)\n",
    "\n",
    "        images[filename]= img\n",
    "\n",
    "labels= open(PATH2, newline=\"\\n\").readlines()\n",
    "\n",
    "print(\"The first image is: \\n\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(images['100009e256.png'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"TEMP LABELS FOR 100 IMAGES\"\"\"  ###remove when using full data\n",
    "lab_1k= labels[0:50]\n",
    "\n",
    "dat_short=[]\n",
    "for k, v in images.items():\n",
    "\n",
    "        dat_short.append(np.array(images[k]))\n",
    "        \n",
    "dat_short=np.array(dat_short).reshape((100,3,size[0], size[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'60ee748793.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-41e27bb8a480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mval_ids\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-41e27bb8a480>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mval_ids\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '60ee748793.png'"
     ]
    }
   ],
   "source": [
    "\"\"\"SEGMENTING DATA BY IDS FOR TRAIN, TEST AND VALIDATION\"\"\"\n",
    "\n",
    "PATH3= \"C:/Users/z003tp7e/Desktop/im2latex_data/\"\n",
    "train= open(PATH3 + \"/\" + \"im2latex_train.lst\").readlines()\n",
    "test= open(PATH3 + \"/\" + \"im2latex_test.lst\").readlines()\n",
    "val= open(PATH3 + \"/\" + \"im2latex_validate.lst\").readlines()\n",
    "\n",
    "###images (input) ids corresponding to each id in train, test, and val data\n",
    "train_ids= [l.split(\" \")[1] for l in train]\n",
    "test_ids= [l.split(\" \")[1] for l in test]\n",
    "val_ids= [l.split(\" \")[1] for l in val]\n",
    "\n",
    "X_train= [images[(tid + \".png\")] for tid in train_ids]\n",
    "X_test= [images[(tid + \".png\")] for tid in test_ids]\n",
    "X_val= [images[(tid + \".png\")] for tid in val_ids]\n",
    "\n",
    "###labels ids corresponding to each image in the train, test and val data\n",
    "\n",
    "train_ids= [l.split(\" \")[0] for l in train]\n",
    "test_ids= [l.split(\" \")[0] for l in test]\n",
    "val_ids= [l.split(\" \")[0] for l in val]\n",
    "\n",
    "###split labels corresponding to ids in train, val and test\n",
    "\n",
    "y_train= [labels[int(i)] for i in train_ids]\n",
    "y_test= [labels[int(i)] for i in test_ids]\n",
    "y_val= [labels[int(i)] for i in val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Use latex vocabulary and create char_to_idx, idx_to_char dictionaries\"\"\"\n",
    "PATH3= \"C:/Users/z003tp7e/Desktop/im2latex_data/\"\n",
    "PATH4= \"C:/Users/z003tp7e.AD001/Downloads/im2latex-tensorflow-master/im2markup\"\n",
    "PATH5= \"C:/Users/z003tp7e.AD001/Desktop/im2latex_data\"\n",
    "\n",
    "vocab = open(PATH4 +\"/latex_vocab.txt\").readlines()\n",
    "formulae = open(PATH4 +\"/formulas.norm.lst\",'r').readlines()\n",
    "char_to_idx = {x.split('\\n')[0]:i for i,x in enumerate(vocab)}\n",
    "# print len(char_to_idx)\n",
    "\n",
    "char_to_idx['#UNK'] = len(char_to_idx)\n",
    "char_to_idx['#START'] = len(char_to_idx)\n",
    "char_to_idx['#END'] = len(char_to_idx)\n",
    "idx_to_char = {y:x for x,y in char_to_idx.items()}\n",
    "\n",
    "print (\"The number of latex chars are:\", len(vocab))\n",
    "#len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Coverting formulae to Sequences of int\"\"\"\n",
    "\n",
    "def formula_to_indices(formula):\n",
    "    formula = formula.split(' ')\n",
    "    res = [0]\n",
    "    for token in formula:\n",
    "        if token in char_to_idx:\n",
    "            res.append( char_to_idx[token] + 3 )\n",
    "        else:\n",
    "            res.append(2)\n",
    "    res.append(1)\n",
    "    return res\n",
    "\n",
    "formulas = list(map( formula_to_indices, formulae))\n",
    "#lab_50= ([np.array(l) for l in (list(formulas)[0:50])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###zero-pad the formulas\n",
    "lab50=formulas[0:100]\n",
    "maxlen= max(len(lab50[i]) for i in range(len(lab50)))\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "lab_0_padded= pad_sequences(lab50, maxlen)\n",
    "#lab_0_padded= lab_0_padded.reshape(lab_0_padded.shape[0], lab_0_padded.shape[1], 1)\n",
    "print(lab_0_padded.shape)\n",
    "\n",
    "lab_one_hot= np_utils.to_categorical(np.array(lab_0_padded))\n",
    "lab_one_hot=lab_one_hot.reshape(lab_0_padded.shape[0], lab_0_padded.shape[1], lab_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_decoder(out_dim,attention= True):\n",
    "    \n",
    "    model.add(LSTM(out_dim, activation= 'relu', return_sequences=True) ) ##output_dim is the dimension of the output of enc\n",
    "    # with a Sequential model\n",
    "    get_3rd_layer_output = K.function([model.layers[0].input],[model.layers[3].output])\n",
    "    layer_output = get_3rd_layer_output([x])[0]  ##is v~ matrix in the paper...\n",
    "              \n",
    "    if attention:\n",
    "              pass\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"BUILDING THE NNET...\n",
    "   i/p--> CNN layers--> ENCODER --> ATTENTION --> DECODER\"\"\"\n",
    "\n",
    "model= Sequential()\n",
    "\n",
    "#CNN PART\n",
    "filter_no_lastlayer= 10                                                                                 #should be 512\n",
    "model.add(Conv2D(60, (3,3), input_shape=(3,size[0], size[1]), padding= \"same\", activation=\"relu\", use_bias=True)) #64 , TRY TO REDUCE SIZE OF IMAGES BY NOT INCLUSING PADDING\n",
    "#model.add(Conv2D(12, (3,3), padding= \"same\", activation=\"relu\", use_bias=True))                          #128\n",
    "#model.add(Conv2D(256, (3,3), padding= \"same\", activation=\"relu\", use_bias=True))                          #256\n",
    "#model.add(Conv2D(256, (3,3), padding= \"same\", activation=\"relu\", use_bias=True))                          #256\n",
    "#model.add(MaxPooling2D(pool_size=(2,1), data_format= \"channels_first\"))                                   #mp(2,1)\n",
    "#model.add(Conv2D(512, (3,3), padding= \"same\", activation=\"relu\", use_bias=True))                          #512\n",
    "#model.add(MaxPooling2D(pool_size=(1,2), data_format= \"channels_first\"))                                   #mp(1,2)\n",
    "model.add(Conv2D(filter_no_lastlayer, (3,3), padding= \"same\", activation=\"relu\", use_bias=True)) ##produces 512 2-d feature maps--> each enc vec is 512 dimensional\n",
    "#print(model.layers)\n",
    "\n",
    "# last_cnn_output = K.function([model.layers[0].input],[model.layers[1].output])\n",
    "# encoder_mat = last_cnn_output([dat_short])[0]  ##is v~ matrix in the paper...\n",
    "# print(encoder_mat.shape)\n",
    "\n",
    "#x is the data to get output on: x=data\n",
    "\n",
    "#beam-search\n",
    "cnn_out= model.predict(dat_short)\n",
    "print(\"The shape of the matrix of the last CNN Layer is:\", cnn_out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"SEQ2SEQ MODEL\"\"\"\n",
    "\n",
    "#params\n",
    "cnn_out=cnn_out.reshape(100, size[0]*size[1],10)\n",
    "xtr= cnn_out[0:90]\n",
    "ytr= lab_one_hot[0:90]\n",
    "xte=cnn_out[90:100]\n",
    "yte=lab_one_hot[90:100]\n",
    "hid_dim=10\n",
    "depth= 2 ##no of LSTM layers in seq2seq\n",
    "\n",
    "model_s2s= Sequential()\n",
    "model_s2s = AttentionSeq2Seq(input_dim=filter_no_lastlayer, input_length=size[0]*size[1], hidden_dim=hid_dim, output_length=maxlen, output_dim=lab_one_hot.shape[2], depth=depth)\n",
    "model_s2s.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "#model_s2s.fit(cnn_out, lab50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_s2s.fit(xtr, ytr)\n",
    "#pred= model.predict(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred= model_s2s.predict(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.shape\n",
    "idx= np.argmax(pred[2], axis=1)\n",
    "\n",
    "res_1= [idx_to_char[idx[i]] for i in range(len(idx))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(pred[3], axis=1)\n",
    "res_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images['100da50051.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
    " \n",
    "def keras_rnn_predict(samples, empty=empty, rnn_model=model, maxlen=maxlen):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)\n",
    "    return rnn_model.predict(data, verbose=0)\n",
    " \n",
    "def beamsearch(predict=keras_rnn_predict,\n",
    "               k=1, maxsample=400, use_unk=False, oov=oov, empty=empty, eos=eos):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "     \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[empty]]\n",
    "    live_scores = [0]\n",
    " \n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    " \n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    " \n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    " \n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    " \n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "         \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    " \n",
    "    return dead_samples + live_samples, dead_scores + live_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.rnn' has no attribute 'AttentionWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-eb7687c9496d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttentionWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.contrib.rnn' has no attribute 'AttentionWrapper'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "cell= tf.contrib.rnn.AttentionWrapper(BasicLSTMCell(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
